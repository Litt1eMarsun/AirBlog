---
title: 视觉大模型微调
urlname: oxokg5t7w4612cg4
date: '2025-09-20 01:34:42'
updated: '2025-10-10 15:49:34'
cover: 'https://cdn.nlark.com/yuque/0/2025/png/43288584/1756486547256-3e53e739-03d0-41b9-86bf-294ff8678b54.png'
description: 1. VLM论文阅读1.1. Idefics始祖了感觉，可以看看思路1.2. Qwen2.5-VL1.3. Moondream1.4. H2OVL-Mississippi1.5. Apollo1.6. smolVLM阅读参数平衡塔——探究了下在小模型当中视觉模块和语言模块的平衡策略看第一张图，...
---
## VLM论文阅读
### <font style="color:rgba(0, 0, 0, 0.85);background-color:rgb(249, 250, 251);">Idefics</font>
始祖了感觉，可以看看思路

### <font style="color:rgba(0, 0, 0, 0.85);background-color:rgb(249, 250, 251);">Qwen2.5-VL</font>
### <font style="color:rgba(0, 0, 0, 0.85);background-color:rgb(249, 250, 251);">Moondream</font>
### <font style="color:rgba(0, 0, 0, 0.85);background-color:rgb(249, 250, 251);">H2OVL-Mississippi</font>
### <font style="color:rgba(0, 0, 0, 0.85);background-color:rgb(249, 250, 251);">Apollo</font>
### smolVLM阅读
参数平衡塔——探究了下在小模型当中视觉模块和语言模块的平衡策略

![](https://cdn.nlark.com/yuque/0/2025/png/43288584/1759038031418-0b6692f0-5e88-4ced-9df2-a748cfd53b57.png)

看第一张图，黄色是0.4b的视觉模块大小搭配，蓝色是0.09视觉encoder大小的编码器。在0.5B的语言模型大小的情况下，搭配0.09B[1:10]的视觉模块比搭配0.4B[1:1]模块的encoder效果好得多。0.8B的语言模型大小搭配0.4B的就会比之前好很多





### 
## VLM微调——SmolVlm和qwen合并微调
### 简单了解一下VLM
<font style="color:rgb(38, 38, 38);">分为两种主要架构Unified Embedding Decoder Architacture</font>

<font style="color:rgb(38, 38, 38);">就是通过encoder处理成多个token,之后把token当作是embedding，和文字一起放到LLM当中</font>

<font style="color:rgb(38, 38, 38);">第二种架构是交叉混合注意力</font>

### SmolVLM
<font style="color:rgb(38, 38, 38);">数据流：</font>

#### Processer
<font style="color:rgb(38, 38, 38);">processer的逻辑大概如下</font>

```python
def data_collate_fix2k(examples, processor, device, max_length=2048):
    batch_text = []
    batch_image = []
    for example in examples:
        images = example["images"][:1]  # 只允许一张图，不然显存压力太大
        batch_image.append(images)
        image_num = len(images)
        chat_texts = example["texts"][0]
        messages = [
            {
                "role": "user",
                "content": [{"type": "image"}] * image_num
                + [{"type": "text", "text": chat_texts["user"]}],
            },
            {
                "role": "assistant",
                "content": [{"type": "text", "text": chat_texts["assistant"]}],
            },
        ]
        text = processor.apply_chat_template(
            messages, enable_thinking=False, add_generation_prompt=False
        )

        batch_text.append(text)

    batch = processor(
        text=batch_text,
        images=batch_image,
        max_length=max_length,
        return_tensors="pt",
        padding="max_length",
        truncation=True,
    )
    labels = batch["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100
    labels[labels == processor.image_token_id] = -100
    batch["labels"] = labels
    return batch.to(device, dtype=torch.bfloat16)
```

<font style="color:rgb(38, 38, 38);">  
</font>

#### 文字部分：
`<font style="color:rgb(38, 38, 38);">原始输入(messages, image)</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">→</font>

**<font style="color:rgb(38, 38, 38);">Jinja 渲染 chat_template（变成一个字符串 prompt）</font>**<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">→</font>`<font style="color:rgb(38, 38, 38);">tokenizer</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">把字符串转成</font>`<font style="color:rgb(38, 38, 38);">input_ids</font>`<font style="color:rgb(38, 38, 38);">（其中包含 image 占位 token id） +</font>**<font style="color:rgb(38, 38, 38);">processor</font>**<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">把图片转成</font>`<font style="color:rgb(38, 38, 38);">pixel_values</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">→</font>`<font style="color:rgb(38, 38, 38);">data_collator</font>`<font style="color:rgb(38, 38, 38);">/</font>`<font style="color:rgb(38, 38, 38);">collate_fn</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">组 batch，构造</font>`<font style="color:rgb(38, 38, 38);">input_ids, attention_mask, pixel_values, labels</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">→</font>`<font style="color:rgb(38, 38, 38);">model.forward</font>`<font style="color:rgb(38, 38, 38);">：vision encoder → connector →</font>**<font style="color:rgb(38, 38, 38);">把 image embeddings 替换/插入到文本 embedding 流里</font>**<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">→ text model（Qwen）前向 → logits → loss（注意对 image 占位处做 mask）。</font>

##### Jinja 渲染 —— 什么时候发生？
+ **<font style="color:rgb(38, 38, 38);">发生在预处理/tokenization 阶段</font>**<font style="color:rgb(38, 38, 38);">，也就是在调用 tokenizer/processor 之前或之时。</font>
+ <font style="color:rgb(38, 38, 38);">常见时机：在</font>`<font style="color:rgb(38, 38, 38);">dataset.map(preprocess_fn)</font>`<font style="color:rgb(38, 38, 38);">、或在</font>`<font style="color:rgb(38, 38, 38);">collate_fn</font>`<font style="color:rgb(38, 38, 38);">（data collator）里，对每条样例做</font>`<font style="color:rgb(38, 38, 38);">rendered_text = apply_chat_template(messages)</font>`<font style="color:rgb(38, 38, 38);">。</font>
+ `<font style="color:rgb(38, 38, 38);">apply_chat_template</font>`<font style="color:rgb(38, 38, 38);">（名字会有变）内部会用 Jinja 渲染模板，把</font>`<font style="color:rgb(38, 38, 38);">messages</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">渲染成一个</font>**<font style="color:rgb(38, 38, 38);">普通字符串 prompt</font>**<font style="color:rgb(38, 38, 38);">（这个字符串里已包含像</font>`<font style="color:rgb(38, 38, 38);"><image></font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">之类的占位文本）。</font>
+ <font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">Jinja 渲染</font>**<font style="color:rgb(38, 38, 38);">是文本层面的</font>**<font style="color:rgb(38, 38, 38);">。渲染后得到的是纯文本（可能包含</font>`<font style="color:rgb(38, 38, 38);"><image></font>`<font style="color:rgb(38, 38, 38);">、</font>`<font style="color:rgb(38, 38, 38);"><row_1_col_1></font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">这些 token 字符串）。</font>

##### tokenizer（文本切分） —— 把渲染后的字符串变成`input_ids`
<font style="color:rgb(38, 38, 38);">对于不一致的特殊token,可以改tokenizer，也可以改templet</font>

<font style="color:rgb(38, 38, 38);"></font><font style="color:rgb(38, 38, 38);">调用</font>`<font style="color:rgb(38, 38, 38);">tokenizer(rendered, return_tensors=...)</font>`<font style="color:rgb(38, 38, 38);">，把渲染后的字符串转成</font>`<font style="color:rgb(38, 38, 38);">input_ids, attention_mask</font>`<font style="color:rgb(38, 38, 38);">。</font>

<font style="color:rgb(38, 38, 38);"></font><font style="color:rgb(38, 38, 38);">tokenizer 必须能识别渲染字符串里的特殊 token</font>

<font style="color:rgb(38, 38, 38);"></font><font style="color:rgb(38, 38, 38);">如果 Qwen tokenizer 原本不包含这些特殊 token，流程里需要先做</font>`<font style="color:rgb(38, 38, 38);">tokenizer.add_special_tokens({"additional_special_tokens": [...]})</font>`<font style="color:rgb(38, 38, 38);">，并且</font>`<font style="color:rgb(38, 38, 38);">model.resize_token_embeddings(len(tokenizer))</font>`<font style="color:rgb(38, 38, 38);">。</font>

<font style="color:rgb(38, 38, 38);">这里处理好的input_ids中会有为图片预留的特殊token，等下面的图像处理好之后，token会被替换为下面的多个图像token</font>

#### 图像部分
![](https://cdn.nlark.com/yuque/0/2025/png/43288584/1756486547256-3e53e739-03d0-41b9-86bf-294ff8678b54.png)

#### Image splitting
<font style="color:rgb(38, 38, 38);">输入的图片通过Splitting变成不同的patch和整图，当然这里的所有的图像是不是在encoder当中还会被打成patching?——是的，这里是预切分</font>

### 过渡部分
#### batch 组装（collate_fn / data_collator）
+ <font style="color:rgb(38, 38, 38);">把若干样例渲染/tokenize的 output 组合成 batch dict：</font>

```python
batch = {
  "input_ids": torch.tensor([...]),
  "attention_mask": ...,
  "pixel_values": torch.tensor([...]),
  "labels": ...            # 通常文本 tokens 的 label，其中文本以外（image占位）设为 -100
}
```

**<font style="color:rgb(38, 38, 38);">labels 的处理很重要</font>**<font style="color:rgb(38, 38, 38);">：把 image 占位所对应的</font>`<font style="color:rgb(38, 38, 38);">input_ids == image_token_id</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">的位置在 labels 中置为</font>`<font style="color:rgb(38, 38, 38);">-100</font>`<font style="color:rgb(38, 38, 38);">，这样 loss 不会在这些位置计算。</font>

<font style="color:rgb(38, 38, 38);">在训练语言模型时，loss（交叉熵）只在</font>**<font style="color:rgb(38, 38, 38);">预测 token 的位置</font>**<font style="color:rgb(38, 38, 38);">上计算。但是多模态输入里，我们会在文本序列中插入一些特殊的</font>**<font style="color:rgb(38, 38, 38);">图像占位符 token</font>**<font style="color:rgb(38, 38, 38);">（比如</font>`<font style="color:rgb(38, 38, 38);"><image></font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">或</font>`<font style="color:rgb(38, 38, 38);"><vision_start> ... <vision_end></font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">之类）。这些 token 本质上不是需要模型预测的“语言”，而是一个提示信号：告诉 backbone“这里对应一张图像的 embedding 会插进来”。如果不特殊处理，loss 就会强制模型去“预测”</font>`<font style="color:rgb(38, 38, 38);"><image></font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">这个 token —— 这没有意义，反而会污染训练。</font>

<font style="color:rgb(38, 38, 38);">sft中无所谓，user的全部要屏蔽不算loss</font>

```python
def forward(input_ids, pixel_values, attention_mask):
    # 1) 文本 embedding（通常先把 input_ids 转为 embeddings）
    text_emb = text_model.embeddings(input_ids)   # shape (B, L, D)

    # 2) 视图编码器
    image_feats = vision_model(pixel_values)      # e.g., (B, C, H', W') -> pooled patches

    # 3) connector 映射成若干 image_token embeddings
    img_tokens = connector(image_feats)           # shape (B, N_img, D)

    # 4) 找到 input_ids 中 image_token_id 的索引位置，
    #    并把对应位置的 text_emb 替换为 img_tokens
    for b in range(B):
        pos = (input_ids[b] == image_token_id).nonzero()
        # 变体 A: 如果 pos 个数 == N_img: 直接一对一替换
        # 变体 B: 如果有单个占位符，则 forward 会把该单占位符在内部“展开”为 N_img 个 embeddings，
        #         并相应调整 attention/position（实现更复杂）
        text_emb[b, pos, :] = img_tokens[b]   # 或者执行展开插入操作

    # 5) 把 inputs_embeds 交给 text_model（Qwen）
    outputs = text_model(inputs_embeds=text_emb, attention_mask=attention_mask)
    logits = lm_head(outputs.last_hidden_state)
    return logits
```

### LLM部分
#### Vision encoder
<font style="color:rgb(38, 38, 38);">encoder部分</font>

#### Pixel shuffle
![](https://cdn.nlark.com/yuque/0/2025/png/43288584/1756486705533-328b6e6a-a07a-4e96-ac86-8181ffee1ae1.png)

<font style="color:rgb(38, 38, 38);">这里操作了encoder之后的特征图，在保证元素数量不变的同时，把长宽同意砍下n倍，同时在特征维度上拓宽n^2倍。可以大大减少参数量，原因如下：</font>

<font style="color:rgb(38, 38, 38);">意义如下：</font>

+ <font style="color:rgb(38, 38, 38);">输入图像形状：</font>`<font style="color:rgb(38, 38, 38);">(B, C, H, W)</font>`<font style="color:rgb(38, 38, 38);">，常见</font>`<font style="color:rgb(38, 38, 38);">C=3</font>`<font style="color:rgb(38, 38, 38);">（RGB）。</font>
+ <font style="color:rgb(38, 38, 38);">选择 patch 大小</font>`<font style="color:rgb(38, 38, 38);">P</font>`<font style="color:rgb(38, 38, 38);">（例如 16），把图像切成不重叠的</font>`<font style="color:rgb(38, 38, 38);">P×P</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">小块。</font>
+ <font style="color:rgb(38, 38, 38);">每个 patch 展平成长度</font>`<font style="color:rgb(38, 38, 38);">P*P*C</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">的向量，然后用一个线性层（或</font>`<font style="color:rgb(38, 38, 38);">Conv2d</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">with kernel=P, stride=P）把这个平面向量映射到模型隐藏维</font>`<font style="color:rgb(38, 38, 38);">d</font>`<font style="color:rgb(38, 38, 38);">（比如 768）。这样每个 patch 变成一个长度为</font>`<font style="color:rgb(38, 38, 38);">d</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">的 token embedding。</font>
+ <font style="color:rgb(38, 38, 38);">token 个数 N=(H/P)×(W/P)N = (H/P) \times (W/P)N=(H/P)×(W/P)。Transformer 的注意力复杂度主要随 N2N^2N2 增长。</font>

<font style="color:rgb(38, 38, 38);">token的数量大小对比如下</font>

+ <font style="color:rgb(38, 38, 38);">没有Pixshf版本的</font>

<font style="color:rgb(38, 38, 38);">设：</font>`<font style="color:rgb(38, 38, 38);">H = 224, W = 224, C = 3, P = 16, d = 768</font>`<font style="color:rgb(38, 38, 38);">（这是 ViT-Base 常见组合）。</font>

<font style="color:rgb(38, 38, 38);">计算每维 token 数：</font>

    - <font style="color:rgb(38, 38, 38);">行/列上 patch 个数 = 224÷16=14</font>
    - <font style="color:rgb(38, 38, 38);">总 token 数 N=14×14=196</font>
+ <font style="color:rgb(38, 38, 38);">有Pixshf版本的：</font>

<font style="color:rgb(38, 38, 38);"></font><font style="color:rgb(38, 38, 38);">(1, 3, 224, 224) 变成了</font>`<font style="color:rgb(38, 38, 38);">(1, 12, 112, 112)</font>`<font style="color:rgb(38, 38, 38);">。</font>

<font style="color:rgb(38, 38, 38);">新的 token 计数（仍用 patch size P=16）：</font>

    - <font style="color:rgb(38, 38, 38);">行/列 patch 个数 = 112÷16=7</font>
    - <font style="color:rgb(38, 38, 38);">总 token N′=7×7=49</font>

<font style="color:rgb(38, 38, 38);">可以看出token数减少了很多，但是每个通道维度上升了</font>

<font style="color:rgb(38, 38, 38);">这会导致shf之后的线性层大小上升了</font>

<font style="color:rgb(38, 38, 38);">patch → embedding 的线性层参数量：</font>

+ <font style="color:rgb(38, 38, 38);">权重矩阵形状 d×patch_flat=768×768</font>
+ <font style="color:rgb(38, 38, 38);">参数数量 =768×768+768（加上偏置）。</font>
+ <font style="color:rgb(38, 38, 38);">768×768=589,824差不多这么多参数</font>

<font style="color:rgb(38, 38, 38);">使用shf参数大不了多少</font>

<font style="color:rgb(38, 38, 38);">patch→embedding 的权重参数数：</font>

+ <font style="color:rgb(38, 38, 38);">权重形状 d×3072=768×3072</font>
+ <font style="color:rgb(38, 38, 38);">768×3072=2,359,296（≈ 原来的 4 倍）。线性增长了r*2倍大小</font>

<font style="color:rgb(38, 38, 38);">但是后面的注意力因为token增大会增大很多，这里不具体列举了所以要进行pixel shuffe，按平方倍减小复杂度。</font>

<font style="color:rgb(38, 38, 38);">  
</font>

### 拼接开始
<font style="color:rgb(38, 38, 38);">详细参考：</font>

[https://github.com/datawhalechina/happy-llm/blob/main/Extra-Chapter/vlm-concatenation-finetune/README.md](https://github.com/datawhalechina/happy-llm/blob/main/Extra-Chapter/vlm-concatenation-finetune/README.md)

<font style="color:rgb(31, 35, 40);">小多模态模型SmolVLM2，可以做到端侧1GB显存推理。虽然有极其强大的视觉文本理解能力，但是模型却无法理解中文，于是打算中文小模型扛把子Qwen3与SmolVLM2直接微调拼接。</font>

<font style="color:rgb(31, 35, 40);">微调的主要思路：</font>

<font style="color:rgb(38, 38, 38);">因为</font><font style="color:rgb(25, 27, 31);">SmolVLM紧密遵循了</font>[Idefics3](https://zhida.zhihu.com/search?content_id=255609186&content_type=Article&match_order=1&q=Idefics3&zhida_source=entity)<font style="color:rgb(25, 27, 31);">的架构，在使用transformers时采用了相同的实现。然而，存在一些关键差异：</font>

+ <font style="color:rgb(25, 27, 31);">用SmolLM2 1.7B替换了Llama 3.1 8B作为语言主干。——意味着可以把SmolVLM2替换为中文的Qwen</font>
+ <font style="color:rgb(25, 27, 31);">更积极地压缩修补后的视觉信息，使用像素洗牌策略将信息减少9倍，而idefics3为4倍。——具体为什么shuffle先不看</font>
+ <font style="color:rgb(25, 27, 31);">使用384*384的Patch，而不是364x364，因为384可以被3整除，这对于像素洗牌策略是必要的。</font>
+ <font style="color:rgb(25, 27, 31);">为此，更改视觉主干以使用形状优化的SigLIP，其Patch为384x384像素，内部Patch为14x14。</font>

<font style="color:rgb(25, 27, 31);">（虽然差异没有研究完）但是主要改进点也围绕着这些元素进行改进</font>

![](https://cdn.nlark.com/yuque/0/2025/png/43288584/1756478277883-412b6921-d532-4d72-b7f5-59bff25f55e2.png)

1. <font style="color:rgb(31, 35, 40);">调整SmolVLM2的“上下文控制格式”，使得其与Qwen3兼容。</font>
2. <font style="color:rgb(31, 35, 40);">将模型的文本部分直接从SmolLM2换成Qwen3-0.6B，包括其文本tokenizer和词嵌入、文本模型、以及模型最后输出的语言模型头（LM Head）。</font>
3. <font style="color:rgb(31, 35, 40);">需要重新初始化特征映射层的MLP，从768->576的单层神经网络改成768->1024的单层神经网络即可。</font>

### <font style="color:rgb(31, 35, 40);">第一处改动：SmolVLM2的Tokenizers部分</font>
<font style="color:rgb(31, 35, 40);">首先需要改动的就是需要改动的是SmolVLM2的Tokenizers部分，这里面主要是涉及两个问题：</font>

#### <font style="color:rgb(31, 35, 40);">第一个问题</font>
<font style="color:rgb(38, 38, 38);">在多模态 VLM 里，</font>**<font style="color:rgb(38, 38, 38);">文本序列里需要一个“锚点”来占位</font>**<font style="color:rgb(38, 38, 38);">，告诉数据管道：“等会儿在这里把图像的视觉 token 插进来”。</font>

<font style="color:rgb(31, 35, 40);">要将SmolVLM2用于指示图像位置的特殊令牌（Special Token）加入到Qwen3的Tokenizer当中，这么做的目的是防止SmolVLM2的图像Token</font>`<font style="color:rgb(31, 35, 40);"><image></font>`<font style="color:rgb(31, 35, 40);">被切分为</font>`<font style="color:rgb(31, 35, 40);"><</font>`<font style="color:rgb(31, 35, 40);">、</font>`<font style="color:rgb(31, 35, 40);">image</font>`<font style="color:rgb(31, 35, 40);">、</font>`<font style="color:rgb(31, 35, 40);">></font>`<font style="color:rgb(31, 35, 40);">三块。幸运的是，Qwen3本身在Tokenizers中预留了未来用于多模态的特殊特殊令牌</font>`<font style="color:rgb(31, 35, 40);"><|image_pad|></font>`<font style="color:rgb(31, 35, 40);">。因此读者直接使用了</font>`<font style="color:rgb(31, 35, 40);"><|image_pad|></font>`<font style="color:rgb(31, 35, 40);">代替了</font>`<font style="color:rgb(31, 35, 40);"><image></font>`<font style="color:rgb(31, 35, 40);">。用于在文本中预留图像特征的插入点。</font>

```python
[请, 描述, 这张, <|image_pad|>]
```

<font style="color:rgb(38, 38, 38);">如果你随便用字符串</font><font style="color:rgb(38, 38, 38);"> </font>`<font style="color:rgb(38, 38, 38);"><image></font>`<font style="color:rgb(38, 38, 38);">，常见 BPE/byte-level BPE 分词器会把它切成多个子词（例如</font><font style="color:rgb(38, 38, 38);"> </font>`<font style="color:rgb(38, 38, 38);"><</font>`<font style="color:rgb(38, 38, 38);">、</font>`<font style="color:rgb(38, 38, 38);">image</font>`<font style="color:rgb(38, 38, 38);">、</font>`<font style="color:rgb(38, 38, 38);">></font>`<font style="color:rgb(38, 38, 38);">），这样</font>**<font style="color:rgb(38, 38, 38);">既不能当成一个原子占位符</font>**<font style="color:rgb(38, 38, 38);">，也不方便后续把 N 个视觉 token 替换进去；训练/推理阶段还会出现位置对不齐、mask 不一致等问题。</font>

<font style="color:rgb(38, 38, 38);">因此，各家都会</font>**<font style="color:rgb(38, 38, 38);">预留“不会被再次切分”的特殊 token</font>**<font style="color:rgb(38, 38, 38);">（在词表里有单独 ID，模型把它当成一个原子）。Qwen 系列在 tokenizer 里</font>**<font style="color:rgb(38, 38, 38);">预留了多模态相关的占位符</font>**<font style="color:rgb(38, 38, 38);">，最常见的就是：</font>

+ `<font style="color:rgb(38, 38, 38);"><|vision_start|> … <|vision_end|></font>`<font style="color:rgb(38, 38, 38);">：包住一段视觉 token 的“开始/结束”哨兵；</font>
+ `**<font style="color:rgb(38, 38, 38);"><|image_pad|></font>**`<font style="color:rgb(38, 38, 38);">：图片占位（在处理管道里会被视觉 encoder 产出的若干 token替换/扩展）；</font>
+ <font style="color:rgb(38, 38, 38);">还预留了</font><font style="color:rgb(38, 38, 38);"> </font>`<font style="color:rgb(38, 38, 38);"><|video_pad|>、<|vision_pad|></font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">等。这些即便在纯文本模型里也能在</font><font style="color:rgb(38, 38, 38);"> </font>`<font style="color:rgb(38, 38, 38);">tokenizer.all_special_tokens</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">里看到（是“为多模态做预留”）。</font>

**<font style="color:rgb(38, 38, 38);">没有研究为什么是一个special token而不是一个段落的specialtoken</font>**

<font style="color:rgb(38, 38, 38);">当然如果没有这种正好替换的情况，比如说新定义了一个token （比如</font><font style="color:rgb(38, 38, 38);"> </font>`<font style="color:rgb(38, 38, 38);"><image_left_eye_bbox></font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">这类自造标记）才需要： 但这么做会改变嵌入矩阵大小，</font>**<font style="color:rgb(38, 38, 38);">通常需要再微调</font>**<font style="color:rgb(38, 38, 38);">，所以</font>**<font style="color:rgb(38, 38, 38);">优先复用</font>**<font style="color:rgb(38, 38, 38);">官方已预留的 token。</font>

<font style="color:rgb(38, 38, 38);">另外一个坑：自己添加时要确保这个 token</font><font style="color:rgb(38, 38, 38);"> </font>**<font style="color:rgb(38, 38, 38);">不要被正则/空白归一化“吞掉”或切分</font>**<font style="color:rgb(38, 38, 38);">。HF 的</font><font style="color:rgb(38, 38, 38);"> </font>`<font style="color:rgb(38, 38, 38);">AddedToken</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">有</font><font style="color:rgb(38, 38, 38);"> </font>`<font style="color:rgb(38, 38, 38);">lstrip/rstrip</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">等选项，相关问题（“前面没空格就被切了”）在 tokenizers issue 里也有人踩过。</font>[GitHub](https://github.com/huggingface/tokenizers/issues/1408?utm_source=chatgpt.com)

<font style="color:rgb(38, 38, 38);">TIPs: 如果生成阶段出现了这些特殊 token，解码时</font><font style="color:rgb(38, 38, 38);"> </font>`<font style="color:rgb(38, 38, 38);">skip_special_tokens=True</font>`<font style="color:rgb(38, 38, 38);"> </font><font style="color:rgb(38, 38, 38);">或者写个后处理把它们去掉即可。</font>

#### <font style="color:rgb(31, 35, 40);">第二个问题是：</font>
<font style="color:rgb(31, 35, 40);">SmolVLM2的chat_template和Qwen3的chat_template差别极大。chat_template的作用是通过格式化文本让模型清楚知道不同Token所代表的背景信息。用最近比较流行的话来说就是“上下文工程”（Context Engineering）。</font>

