---
title: 日常实习记录
urlname: uuer0g2gyn3nuw4w
date: '2025-10-23 11:55:59'
updated: '2025-11-28 14:27:25'
cover: 'https://cdn.nlark.com/yuque/0/2025/png/43288584/1761736324880-da9f485c-e3db-440d-aed9-bfe89646be5a.png'
description: Host docker_syj HostName 10.160.4.180 Port 8080 User root11.28broswer-use浏览器交互的工具使用调研目标agent流程？用了哪些工具？memory如何做管理？
---
```plain
Host docker_syj
HostName 10.160.4.180
Port 8080
User root
```

## 11.28
1. broswer-use浏览器交互的工具使用调研
    1. 目标
        1. agent流程？
        2. 用了哪些工具？
        3. memory如何做管理？
    2. 过程
        1. 调一个简单的问题
        2. debug，看调用过程

## 11.25
### 能力：
1. 简单了解了CDP的概念

### 工作上：
1. 实现了broswer-use的使用，网络调通
    1. 解决<font style="color:rgb(204, 204, 204);background-color:rgb(24, 24, 24);">问题已经从 “LLM 请求超时” 变成了 “浏览器 CDP 连接失败”。</font>

 ![](https://cdn.nlark.com/yuque/0/2025/png/43288584/1764080371261-b55c668c-27ff-427c-820e-50613f4f04c5.png)

## ![](https://cdn.nlark.com/yuque/0/2025/png/43288584/1764080390790-489866a4-b59b-4981-91ae-adb6bbfd0f52.png)
2. 实现了另一个LLM接口的调用，openai没钱了，知道了这里的LLMclinet都是异步封装的。

## 11.18
### 能力：
1. 学会了从react agent角度优化deepsearch流程中的某一个部分（子图）
2. 了解了预训练中的数据配比，训练策略，退火时训练。
3. 了解了postraining中的一些eval标准

### 工作上：
1. 意识到了extrator中的数据噪声/无法召回的问题，并在脑内建立了此时的图谱（v0.1)
2. 意识到可以用多轮提取的方式，提取jina返回的结果中的信息，降低噪声影响（已废弃）
3. 意识到v0.1版本中可能存在的，大模型会将instruction中的substep当作主要的提取方向，从而忽略了作为列表的query所需要的主要调研方向，从而总结出错误的结论。
4. 想到了v0.2版本，即在把jina作为工具，让大模型自己决定何时调用，并实现。
5. 找到问题：模型的输出结构可能会出现错误（answer被列表套上了[]），在后续加了个过滤，过滤正确的结构
6. 找到问题：目前模型调用工具的积极性不高的问题——可能是context出现问题，可能是model是mini，趋向于不调用工具
    1. model可以通过configation类来统一管理。从而在需要的位置配置特定的模型，来在当前节点使用更合适的模型

## 11.17 
### 能力：
1. git PR规范掌握+常用命令掌握
2. AE的内部调研了一下原因，知道了大概为什么AE_guidance不好使，可以继续做实验探索一下：
3. 了解python的泛型机制，如何实现python泛型，typing类
4. 了解agent编程规范

### 工作：
1. prompt管理规范化
2. 尝试了	
    1. plan阶段的校验（根据老师说效果不好，确实没办法保证，效果也无法量化，跑badcase成本太高）
    2. query数量的限制（依然是效果无法保证，跑实验成本太高）
    3. extract 的prompt改变，尝试增大检索到结果的可能性（额单个badcase样例没看出来什么效果，但是换deepseek能看到效果））

科研：

1. 泡了个case1，纯AE guidance效果仍然烂了。
2. 为什么之前的case能够稍微稳定一点？很奇怪

## 10.13
### 工作上：
+ 调研了问题的badcase，并对badcase进行分类整理，提出了可能可行的修改方案：
+ 调研了地图API，对本模型中所需要的API的核心需求界定了下，并对比了多个API之间的
    - 能力
    - 费用

### 能力上：
+ 熟悉了open-deep-research的逻辑[https://blog.langchain.com/open-deep-research/](https://blog.langchain.com/open-deep-research/)
    - 对deepsearch的问题简单分了类
        * 主要分为：可以并行的问题，即问题可以拆解为多个可以并行进行的substep
        * 串行进行的问题，逻辑紧密，前一步需要后一步的结果
    - 对比了当前架构在open_deep_research
        * deepsearch范式目前依旧是plan\search\write这个pipeline，现在plan主要是多步plan。比如说open deep research，planner用来规划，不过这个规划主要是并行的，如果只需要拆分成单个substep，他可能不会串行的去拆。串行的任务主要交给researcher，让research agent自行去规划，调用tool
        * 我们的架构相当于plan过程中不对任务进行plan并行拆分，而是串行进行拆分，之后搭建了workflow去代替research agent
    - 优点是工作流很稳，而且自由还有就是每个节点可以单独去设计prompt，比如说可以扩充query的个数，并且每个query并发查询，效率会高
    - 缺点就是可能不能完全压榨大模型的潜力，如果大模型自己有规划的能力，还得按照workflow的规划，可能能力会有下降。还有就是不方便回溯，反复查询，这里就定死了，反复查询，回溯，可能会很烧钱？

### 科研上


## 10.12:
### 工作上：


### 学习上：
1. 熟悉了异步编程，其中awati asynic，assynic.run这三个关键字
2. 调研下现有哪些项目，哪些项目适合读
3. 重新熟悉了下AE的训练代码

### 科研上：
1. 训练了t_emb，提高了embedding 的维度，成功跑起来了loss下降合理，但是还是不够低，再提高embedding？
2. 写点验证脚本，跑个带噪推理试试

### 日常：
1. 开题报告申请提交了，但是还没写

## 11.10-11
### 工作上：
1. 写了个版本：探索对gen_query循环的可能性与过程
2. 写了个脚本，对所有没有成功的qa进行了plan，观察plan这个环节上在整个流程中是否存在问题
3. 把每个badcase细致分析，对比golden版本和目前版本plan的区别
4. 通过几个不成功的case简单确认了下，本版本plan除了冗余之外问题不大，性能瓶颈反而位于query返回的结果较少，查询不到的问题
5. 给脚本写了个日志，将所有需要的中间结果打印到日志上

### 学习上：
1. 找了篇huggingface写的报告，训练SmolLM的整个流程，记得同步到笔记上
2. 简单学习了下GRPO，详见小红书笔记，但是没有同步

### 科研上：
1. 把训练的脚本写好了，但是还差t_embedding的嵌入，让gpt写一个版本问问wt

## 11.9
### 科研上：
1. 改下代码，直接guidance 到z当中，看看效果？
2. 带噪的输入到AE当中，看看重建出什么？
3. 让模型拥有 干净序列在对应噪声的情况下，也可以将key_frame扩展到周围的能力
    1. 策划
        1. 训练时：（训练出一个完全重建出输入的AE，并且输入带噪声）
            1. 选择增加t噪声的时间段，应该在运动趋势出现的时候，人形没有出现的时候
                1. 500-800的噪声加噪？
            2. 将周围帧在t时刻加上对应噪声，送入模型当中，重建出对应序列
                1. 看看训练mdm的时候噪声是怎么调度的
                2. 将mdm的噪声调度方案用到训练AE上的加噪过程
            3. 加上t_embedding，让模型能够知道对应噪声的程度，辅助重建
        2. 推理时
            1. 规定时间段将带噪的噪声输入到AE当中
                1. 规定时间段
            2. 正传到AE输出端，和reference（加噪版本）算loss
            3. loss反传到z，更新z并重建
    2. prompt:

整理加噪策略的prompt:

> diffusion在训练的时候是需要加噪的，我现在有个motion diffusion model 需要训练，代码噪声调度及加噪过程如下，我没怎么看懂，能帮我总结一下吗：
>

```python
# Use float64 for accuracy.
        betas = np.array(betas, dtype=np.float64) # 噪声调度参数
        self.betas = betas
        assert len(betas.shape) == 1, "betas must be 1-D"
        assert (betas > 0).all() and (betas <= 1).all()

        self.num_timesteps = int(betas.shape[0])

        # 以下是加噪所需要的参数
        alphas = 1.0 - betas #beta不需要累计，因为beita是每一个step中，加噪使用到的参数
        self.alphas_cumprod = np.cumprod(alphas, axis=0) #所有的alpha参数（累计之后），alpha需要累计的原因是alpha是从初始噪声得到任何一个step加噪图像的方法。
        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1]) # 得到第一个step所需要的参数
        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)
        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)
       # calculations for diffusion q(x_t | x_{t-1}) and others
        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod) 
        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)
        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)
        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)
        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)

        # calculations for posterior q(x_{t-1} | x_t, x_0)
        self.posterior_variance = (
            betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod) #获得去噪所使用的噪声方差参数
        ) 
        # log calculation clipped because the posterior variance is 0 at the
        # beginning of the diffusion chain.
        self.posterior_log_variance_clipped = np.log(
            np.append(self.posterior_variance[1], self.posterior_variance[1:]) 
        )
        self.posterior_mean_coef1 = ( # 利用预测的x0计算后验分布的均值计算参数1 （x0之前的参数）
            betas * np.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)
        )
        self.posterior_mean_coef2 = ( # 后验分布的均值计算参数2 （xt之前的参数）
            (1.0 - self.alphas_cumprod_prev)
            * np.sqrt(alphas)
            / (1.0 - self.alphas_cumprod)
        )
```

之后训练部分加噪的代码如下

```python
def training_losses(self, model, x_start, t, model_kwargs=None, noise=None, dataset=None):
        """
        Compute training losses for a single timestep.

        :param model: the model to evaluate loss on.
        :param x_start: the [N x C x ...] tensor of inputs.
        :param t: a batch of timestep indices.
        :param model_kwargs: if not None, a dict of extra keyword arguments to
            pass to the model. This can be used for conditioning.
        :param noise: if specified, the specific Gaussian noise to try to remove.
        :return: a dict with the key "loss" containing a tensor of shape [N].
                 Some mean or variance settings may also have other keys.
        """

        mask = model_kwargs['y']['mask']

        if model_kwargs is None:
            model_kwargs = {}
        if noise is None:
            noise = th.randn_like(x_start)
        x_t = self.q_sample(x_start, t, noise=noise) # 先加噪
        x_t = self.guide(x_t, t, model_kwargs=model_kwargs, train=True,rep_i = None) # 在训练的时候反传的意义在哪？
        terms = {} 

        model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)

        target = {
            ModelMeanType.PREVIOUS_X: self.q_posterior_mean_variance(
                x_start=x_start, x_t=x_t, t=t
            )[0],
            ModelMeanType.START_X: x_start,
            ModelMeanType.EPSILON: noise,
        }[self.model_mean_type]
        assert model_output.shape == target.shape == x_start.shape  # [bs, njoints, nfeats, nframes]

        terms["rot_mse"] = self.masked_l2(target, model_output, mask) # mean_flat(rot_mse)

        terms["loss"] = terms["rot_mse"]

        return terms
```

其中

```python
def q_sample(self, x_start, t, noise=None):
        """
        Diffuse the dataset for a given number of diffusion steps.

        In other words, sample from q(x_t | x_0).

        :param x_start: the initial dataset batch.
        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.
        :param noise: if specified, the split-out normal noise.
        :return: A noisy version of x_start.
        """
        if noise is None:
            noise = th.randn_like(x_start)
        assert noise.shape == x_start.shape
        return (
            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start
            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)
            * noise
        )
```

> 我现在在训练一个重建运动序列的Autoencoder，但是不同于重建干净的运动序列，我想在使用DecompTrainerV3(object)训练的时候，训练的不是干净的运动序列重建，而是带噪声的运动序列重建。
>
> 噪声的调度策略如下：
>

```python
 alphas = 1.0 - betas #beta不需要累计，因为beita是每一个step中，加噪使用到的参数
        self.alphas_cumprod = np.cumprod(alphas, axis=0) #所有的alpha参数（累计之后），alpha需要累计的原因是alpha是从初始噪声得到任何一个step加噪图像的方法。
        self.alphas_cumprod_prev = np.append(1.0, self.alphas_cumprod[:-1]) # 得到第一个step所需要的参数
        self.alphas_cumprod_next = np.append(self.alphas_cumprod[1:], 0.0)
        assert self.alphas_cumprod_prev.shape == (self.num_timesteps,)
       # calculations for diffusion q(x_t | x_{t-1}) and others
        self.sqrt_alphas_cumprod = np.sqrt(self.alphas_cumprod) 
        self.sqrt_one_minus_alphas_cumprod = np.sqrt(1.0 - self.alphas_cumprod)
        self.log_one_minus_alphas_cumprod = np.log(1.0 - self.alphas_cumprod)
        self.sqrt_recip_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod)
        self.sqrt_recipm1_alphas_cumprod = np.sqrt(1.0 / self.alphas_cumprod - 1)
    def q_sample(self, x_start, t, noise=None):
        """
        Diffuse the dataset for a given number of diffusion steps.

        In other words, sample from q(x_t | x_0).

        :param x_start: the initial dataset batch.
        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.
        :param noise: if specified, the split-out normal noise.
        :return: A noisy version of x_start.
        """
        if noise is None:
            noise = th.randn_like(x_start)
        assert noise.shape == x_start.shape
        return (
            _extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start
            + _extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)
            * noise
        )
```

> 我想要对输入的x在t=500-t=300之间随机加噪，之后将加噪的序列进行重建
>
> 你看看，如何修改代码，完成加噪的过程比较合适.
>
> 同时，我还想给训练的autoencoder加上t_embedding，让模型能够知道对应噪声的程度，辅助重建,如何设计time_embedding的嵌入比较合适？
>



### 工作上：


## 11.5	
1. 在gen_query中需要加入memory，analyze需要加入memory。在两者中加入memory之后优化
2. 

## 11.4
1. 添加工具使用的逻辑
2. 找找目前存在的问题：
    1. 上下文工程几乎没有，在每次Prompt当中并没有给出时间窗口的记忆
    2. 添加功能模型planning的过程中需要调用模型则开始调用，不需要则不调用
        1. state当中添加字段
        2. 增加analyze节点，summry节点
        3. 重新规划图
    3. prompt有点问题，输出格式错误
3. 学习下searchR1

## 11.3
### 工作方面：
#### 理解一下目前的项目流程
Planing-and-solve框架，现在的项目还是比较naive的

##### 状态
定义的状态如下：

```python
class MultiStepState(TypedDict):
    original_question: str
    substeps: Optional[List[str]]  # 由拆分器生成的步骤描述（自然语言）
    current_step_index: int
    history: List[dict]  # 每个元素: {"step": i, "step_desc": "...", "query": "...", "search_results": [...], "answer": "...", "evidence": [...]}
    finished: bool
    final_content: str
# 初始化 state 示例
def initial_state(question: str) -> MultiStepState:
    return MultiStepState(
        original_question=question,
        substeps=None,
        current_step_index=0,
        history=[],
        finished=False,
        final_content=""
    )
```

包括了当前的原始question,分解的planing，当前执行到哪个step了，执行的histroy，是否完成，最终的答案

比较重要的就是histroy，这个包含了以下字段：

```plain
{"step": idx, "step_desc": step_desc, "query": query, "search_results": None, "answer": None, "evidence": None}

```

包括了当前执行的step的id，对应的step内容，生成的query，搜索的结果，模型回答的答案

##### 具体Planing&solve过程
整体的流程如下

```python
ef build_multistep_graph() -> StateGraph:
    # StateGraph 构造器使用 State 类型（这里用 dict 即可）
    graph = StateGraph(MultiStepState)

    # 添加节点
    graph.add_node("decompose", decompose_node)
    graph.add_node("gen_query", gen_query_node)
    graph.add_node("tool_node", tool_node)
    graph.add_node("extract", extract_node)

    # edges: START -> decompose -> gen_query -> tool_node -> extract -> decide -> gen_query ... -> END
    graph.add_edge(START, "decompose")
    graph.add_edge("decompose", "gen_query")
    graph.add_edge("gen_query", "tool_node")
    graph.add_edge("tool_node", "extract")

    # conditional edge: after extract decide whether to continue
    def decide_edge(state: MultiStepState):
        return advance_or_end(state)

    graph.add_conditional_edges("extract", decide_edge, ["gen_query", END])

    # compile
    agent = graph.compile()
    return agent
```

先通过decompose进行planning，之后对planning中的某个子step进行query的生成，对应了gen_query这个节点，在生成query之后直接调用tool(查询tool),之后对查询到的结果和其他的状态信息进行extract，获得当前step的最终答案。

如果判断整体流程是否结束，如果不结束继续生成query

1. 整体planning:

```python
def decompose_node(state: MultiStepState) -> MultiStepState:
    """
    输入: state['original_question']
    输出: state['substeps'] = [step1_desc, step2_desc, ...]
    Prompt 模板里强调：每个子步骤独立且可以用一次搜索解决；必须按顺序返回。
    """
    question = state["original_question"]
    prompt = [
        SystemMessage(content="你是一个负责把复杂问题拆解为有序子任务的大模型助手。每个子任务应当是单一信息检索目标，且可以通过一次网络检索得到答案。"),
        HumanMessage(content=(
            "请把下面的问题拆分成若干按顺序执行的子步骤（列成编号列表）。"
            "\n问题：\n" + question +
            "\n要求：每个子步骤应简短、可执行，并且子步骤的输出是下一步的输入。"
            "\n只返回 JSON 数组，例如：[\"step1 描述\", \"step2 描述\", ...]，不要额外解释。"
        ))
    ]
    resp = model.invoke(prompt)
    # 假设模型直接返回一个 JSON 数组文本；在生产需加 robust parsing & fallback
    text = resp.content if isinstance(resp, AIMessage) else str(resp)
    try:
        import json
        steps = json.loads(text)
        if not isinstance(steps, list):
            raise ValueError
    except Exception:
        # 兜底：用简单换行切分（更保守）
        lines = [l.strip() for l in text.splitlines() if l.strip()]
        steps = lines
    print(f"拆分的步骤steps: {steps}")
    state["substeps"] = steps
    state["current_step_index"] = 0
    return state
```

prompt设定的就是把problem按照json格式拆分成多个step，之后返回的应该是AIMessage格式的回答，把text的部分拎出来，每行就是每个step

之后更新下state，substep和currstep，当前node就结束了。

2. query生成

```python
# Node B: 生成针对当前子步骤的搜索 query
def gen_query_node(state: MultiStepState) -> MultiStepState:
    idx = state["current_step_index"]
    step_desc = state["substeps"][idx]
    # Prompt: 让 LLM 输出 1-2 个高质量搜索 query（short），并对为何选择该 query 简短说明
    if idx == 0:
        prompt = [
            SystemMessage(content="你是一个帮助把任务转换为高质量搜索 query 的助手。"),
            HumanMessage(content=(
                    f"当前子步骤（索引 {idx}）：{step_desc}\n"
                    "请只返回 JSON 对象 { \"query\": \"...\" }，其中 \"query\" 应为直接可用于搜索引擎的查询字符串。"
                ))       
        ]
    else:
        last_answer = state["history"][-1]["answer"]
        prompt = [
            SystemMessage(content="你是一个帮助把任务转换为高质量搜索 query 的助手。"),
            HumanMessage(content=(
                    f"上一步的结果是：{last_answer}\n"
                    f"当前子步骤（索引 {idx}）：{step_desc}\n"
                    "请只返回 JSON 对象 { \"query\": \"...\" }，其中 \"query\" 应为直接可用于搜索引擎的查询字符串。"
                ))       
        ]
    resp = model.invoke(prompt)
    text = resp.content if isinstance(resp, AIMessage) else str(resp)
    try:
        import json
        qobj = json.loads(text)
        query = qobj.get("query")
    except Exception:
        # 兜底：直接使用 step_desc 作为 query
        query = step_desc
    state.setdefault("history", [])
    state["history"].append({"step": idx, "step_desc": step_desc, "query": query, "search_results": None, "answer": None, "evidence": None})
    return state
```

首先从state当中获得当前的step_id和当前step_id对应的step，之后将这些组合成prompt送入模型当中，获得json格式的AImessage query。

如果之前有生成的结果，还要把state中history中的answer也放到prompt当中，组合生成最终query

生成query之后依然按照之前的



1. 看badcase，分析原因，定位到badcase原因发生的位置

#### badcase
首先看看badcase中的planning做的怎么样：

1. **task1:**

> '根据世界卫生组织《世界卫生统计》（World Health Statistics）报告，2020、2012两年全球人均预期寿命（总人口）的整数部分分别是多少？8年内增加或减少了多少（保留一位小数）？'
>

对于task1多次生成planning，其中一次效果如下

> 1. 查找并打开世界卫生组织《World Health Statistics 2020》报告（网页或PDF），在报告中找到“全球人均预期寿命（总人口）”的数值并记录完整数值为2020_value。'
> 2. '从步骤1得到的2020_value中取其整数部分并记录为2020_int（输出用于下一步）。'
> 3. '查找并打开世界卫生组织《World Health Statistics 2012》报告（网页或PDF），在报告中找到“全球人均预期寿命（总人口）”的数值并记录完整数值为2012_value。'
> 4. '从步骤3得到的2012_value中取其整数部分并记录为2012_int（输出用于下一步）。'
> 5. '用步骤1的2020_value减去步骤3的2012_value，计算8年内的变化量，保留一位小数并标明增加或减少，记录为change_1dp。'
>

可以看出来planning存在问题

设计的agent是把tool的调用写死了的，也就是不管planning的计划如何，都会生成query并调用，但是planning的2，4明显不需要进行search，这就会返回很多奇怪的搜索结果，浪费没有必要的token：

优化方案：

+ 调整prompt，分步的时候确定每个step是需要搜索信息的，整合信息的步骤不应该出现在Planning非最后一个步骤当中
+ 使用searcho1的架构与Prompt，即把搜索包裹在推理的过程当中，需要搜索的时候生成对应tag调用外部工具进行搜索，不把tool写死

对于Planning中的每个step生成的query对应效果如下：

round1:

> 'World Health Statistics 2020 site:who.int filetype:pdf "life expectancy at birth"'
>

之后拿到的结果是一大段话，具体不打印出来了

模型对此总结的结果是：

'未在所提供的搜索结果文本中找到《World Health Statistics 2020》报告里“全球人均预期寿命（总人口）”的具体数值，故无法记录 2020_value。'

![](https://cdn.nlark.com/yuque/0/2025/png/43288584/1762160319265-fa6a3b72-a543-4456-b978-7b55c60fea60.png)

并给出了evidence(有个小问题就是evidance并没有按照预期的结构进行输出，位置错了）

同时我用gpt5和claude重新检索了一边，确认是检索结果不包含对应信息，而不是模型总结的问题

因此本轮最大的问题是根据本query检索到的结果压根没有需求的信息，按理说应该多次检索/或者检索多个结果并从中挑选

![](https://cdn.nlark.com/yuque/0/2025/png/43288584/1762162485822-ab4f8cf9-9651-41d5-9268-a711c70146b6.png)

后面这个就是因为planning错误导致的无效query

2. **task2:**

****





## 10.30
### 工作方面：
1. 将tao2benchmark整体项目逻辑看了一遍，
2. 熟悉了整个项目的文件框架，也熟悉了运行逻辑。
3. 但是对很多数据结构，设计思路，运行细节（线程池，异步函数等）没有了解，并且没有和hello agent框架做对比，同时没有和其他的benchmark的实现方式对比

## 10.29
### 工作方面
1. 总结了tao2_bench和ACEbench这两个考验agent调用能力的benchmark
2. 对于tao2，
    1. 感觉主要就是带交互，使用了一个专用的agent去模拟客户，并且这个客户也只可以调用api，防止客户出现幻觉的情况，增强可信性。
    2. 在具体实现的时候，给agent涉及一套prompt限制，并告知身份，并给定具体的任务
    3. 在客户agent和给定大模型作为agent的交互结束/达到最大交互次数之后，输出结果，并与







## 10.28
### 工作方面
1. 把之前api的输出整理benchmark做评分
    1. code测试正确率
    2. 剩下的整理成xlxs
    3. 看看qwen3 4b的inference效果如何
2. 阅读下之前发的deepsearch的benchmark/还有文档中提到的其他的benchmark

### 科研方面
### 自学方面
第七章看完，代码熟悉熟悉

## 10.27
### 工作方面：
1. 评测所需环境搭好
    1. vllm安装方式选择，版本选择
        1. 直接Pip不太行
2. 熟悉下模型评测代码框架，新增接口并测试模型
    1. 搭建评测代码，测试api版本的接口
        1. 搭建测试完毕
        2. 正式工程
            1. 所有数据集都要跑吗？
            2. 对模型有没有要求？
                1. 是否使用thinking？
                2. 还有多少钱？
            3. 如何得出结论：本模型的效果如何？
                1. 不同的数据集整理badcase，看看失误在哪
            4. 把跑出来的数据集进行评测
                1. 对于code数据集，使用Judge code
                2. 对于剩下的，跑json2xlxs来跑
            5. 检验跑出来的效果
                1. judge看正确率
                2. 直接输出
    2. 代码换不同平台跑通，并debug，看看效果
        1. 熟悉vllm平台，基本的架构，外层代码调用了解一下
        2. api调用的方式，表层了解一下
        3. local同样
    3. 尝试换不同模型进行debug
3. 熟悉下新的测评benchmark[<font style="color:rgb(31, 35, 40) !important;">xbench-evals</font>](https://github.com/xbench-ai/xbench-evals)<font style="color:rgb(31, 35, 40) !important;">，写下评测计划，如何用之前的评测框架评测一下当前的题目集合</font>

### 科研方面
1. 大概搞懂了flowedit中每个符号的定义
2. 确定出一个可用flowedit的方法
    1. 暂时确定不了，因为Model预测的是v，但是现有baseline并没有直接预测V的
        1. 直接使用x去进行预测，改写公式之类的，调研一下有没有现有方法
3. 指出一个实践方法脉络如下：
    1. 继续探索为什么guidance没办法传导到周围
        1. 问问Gpt可能的原因，我该如何探索
        2. 猜测可能是原始的motion是带噪声的，直接反向传播到原始x0上，对干净x0^有效的梯度没办法作用到带噪声的x0上，所以传播的梯度完全是无效梯度。——问问GPT这个猜想可能吗，如何验证这个猜想，如果验证成功这个猜想应该如何解决
    2. 可能的问题修正
        1. 传到z之后重建出来的作为x0
    3. easy版本先实现
        1. 直接使用窗口函数，给关键帧附近同时进行监督，靠近强监督，不靠近弱监督
    4. mid版本
        1. 重建窗口并送入不断进行Guidance，看看最终会是什么样的
    5. hard版本
        1. 这个版本主要为了解决之前的梯度对带噪的x0无效的方案，同时跟进目前的edit方案，即获得轨迹+guidance
        2. 首先看正常的带噪数据送入AE当中进行重建是什么样的
        3. 对数据集中的数据进行随即加噪，t=500-300 , 用这些数据进行训练AE
        4. edit步骤重新更改，先获得加噪的轨迹，对t=500-300中间的轨迹进行重新加噪，并guidance
    6. devel版本
        1. 上述的混合
    7. 新版本
        1. 调研方案
            1. TFG
            2. CGD
    8. 实现顺序：
        1. easy版本，mid版本和前面所有并行处理
        2. 之后处理hard，devel

### 自学方面：
#### 计划：
1. 阅读之前总结的memory方向和/context enginerring的任意一个综述





## 10.23-10/26总结与展望
### 科研方面
把指定的论文看了，尝试规划了在本方法上应该怎么操作，但是仍然有方法概念上没搞懂的地方，以及与本方法不太适配的地方

下周把老师布置的两个任务干完，首先就是看看flowedit这篇有没有落地的方案

其次就是师兄这边进行落实

### 工作方面
熟悉了下服务器，部署了下测试环境。

本周工作内容较少，工作模式是早上过来要任务，干完之后两三个小时自己

### 自学方面
agent这块大致的方向了解了一下

这周要把agent热门方向的综述都阅读

## 10.26
### 计划：
1. 把每个步骤的原理深入一层
2. 对步骤进行修正，查找遗漏
3. 看看有没有现有框架可以用上flowedit的，找到了和老师沟通下
4. 确定一个可以读的综述论文，并对agent当前方向有个概念
    1. 有哪些研究方向，如何分类？
    2. 有哪些框架？优劣？
    3. 有哪些范式？

### agent综述调研
也是让大模型找了一下，设计了一个prompt，如下：

> <font style="color:rgb(17, 17, 51);background-color:rgb(227, 225, 255);">我是一个小白，因为我想了解了解最新的研究热点，以及agent当前有哪些研究方向，以及有什么可以应用的方面（比如说deep research之类的）所以请你帮我找找最新的agent综述，学术论文吧，最好在2025年以后发表的</font>
>

涵盖了自身意图，并对意图进行拆分。并对意图进行了few-shot，同时加上了限定范围。

prompt可以优化：先对意图给大模型，让大模型进行拆分

比如说agent有哪些方向，agent应用的场景可以直接找大模型调研，并整理作为few-shot

实际上建议多级搜索，观察了下结果，最合适的并不一定是最全的prompt产生的，有可能简短的Prompt也ok

#### 综述集合
**agent benchmark方面：**

1. [https://arxiv.org/abs/2504.19678?utm_source=chatgpt.com](https://arxiv.org/abs/2504.19678?utm_source=chatgpt.com)

 快速对比 LLM 推理能力与 agent 系统的评测/基准，适合想了解评价框架与benchmark的同学。  

**agent workflow方面**

1. [https://arxiv.org/abs/2508.01186?utm_source=chatgpt.com](https://arxiv.org/abs/2508.01186?utm_source=chatgpt.com)
2. [https://www.sciencedirect.com/science/article/pii/S2949855425000516?utm_source=chatgpt.com](https://www.sciencedirect.com/science/article/pii/S2949855425000516?utm_source=chatgpt.com)
+ 出版时间：2025 年 (ScienceDirect) [科学直接](https://www.sciencedirect.com/science/article/pii/S2949855425000516?utm_source=chatgpt.com)
+ 内容：从 “agentic AI” 更宽泛的角度出发，讨论基于 agent 的智能系统（不仅 RL）面临的安全挑战、推理能力、决策能力等。
+ 优点：视角较宽、适合你做 project 时兼顾安全／系统能力提升等“非纯算法”因素。
+ 适合你读的角度：如果你希望项目提升到“不仅能跑通，而且有思考、安全、可部署”层面，这篇会有启发。

**muti agent**

1. [https://link.springer.com/article/10.1007/s10458-023-09633-6?utm_source=chatgpt.com](https://link.springer.com/article/10.1007/s10458-023-09633-6?utm_source=chatgpt.com)

**应用场景方面**

1. deepresearch
+ [https://arxiv.org/abs/2508.12752?utm_source=chatgpt.com](https://arxiv.org/abs/2508.12752?utm_source=chatgpt.com)
2. GUI agent
+ [https://arxiv.org/html/2411.04890v2?utm_source=chatgpt.com](https://arxiv.org/html/2411.04890v2?utm_source=chatgpt.com)

**agentic RL**

1. [https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com](https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com)

综述系统梳理了 “agentic RL”（即把大语言模型/决策主体当作可训练“代理 (agent)”）的研究格局。聚焦能力维度：规划 (planning)、工具使用 (tool use)、记忆 (memory)、推理 (reasoning)、自我改进 (self-improvement)、感知 (perception) 等  

#### 总结：
屎里淘金

Google之前的那篇感觉可以，

[https://github.com/FoundationAgents/awesome-foundation-agents](https://github.com/FoundationAgents/awesome-foundation-agents)

时间稍微晚了点，3个月前更新的，可以一读，主要太长了，太冗余了

剩下的

1. [https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com](https://arxiv.org/abs/2509.02547?utm_source=chatgpt.com)

这篇覆盖也很全，而且还侧重了RL去说

在剩下的要不就太专了，要不就太早了，这两篇读完了要是有需要详细了解的再看看吧。

### 简单阅读agentic RL综述
感觉确实偏RL，虽然对每个结构进行了拆分，大致的说明了agent的重要结构，但是对于基础的一些非RL方法没有怎么提及，感觉想要先读一下才能进入学习。

具体的基础方法，既然已经分过类，并且没有找到合适的综述，就挨个找吧，分类大概如下：

+ context engineering
+ perception
+ planning
+ memory
+ reasoning
+ tooluse
+ self-imporoving
+ multi-agent
+ benchmark

分类标准来源于_hello_agent和RL那片综述，剩下的太多了，没办法好好读，感觉这两篇能覆盖大部分了

#### 总结：
先根据hello agent学学agent的范式，范式简单看一下就行，因为后面memory之类的搭建都会涉及到范式，每次过一遍应该就熟悉了，所以agent范式熟悉下概念就行，不打算动手。这部分大概半周搞定

之后agent每个步骤的深入扩展这里，也就是之前的分类，每个都得找一篇综述看看，在重要的综述这里挑一两个去实践一下，偏向于memory/tooluse/context enginerring，感觉最工程的几个分类，需要观察效果并对比





### 总结：
1. 对于框架感觉学习成本高，对比起来比较复杂，不如先挑一个简单的框架，方便实现后面不同研究方向即可，这个简单做了解，不要深究
2. 对于范式，感觉后续影响也不大，调一两个范式简单过一边就ok了，一开始也不打算学
3. 后续的研究方向还是打算先看看对应方向的综述，之后挑一两个实现一下
4. 下周把每个方向都要看一篇综述，可以不了解方向具体的内容，但是必须了解其发展脉络

## 10.25回顾
### 看了下flow edit这篇论文解读
### 简单列了下算法步骤
## 10.24回顾
### 配了下环境
1. 装了个conda

### 项目整体文件框架
codes下面全是复杂的完整代码例子，应该是用来跑数据集/跑测试的

config下面包含的是main函数运行需要的配置，

包括了跑的数据集，

选用的模型，模型的配置

评测的方式，下面就是

```json
{
        "model":"deepseek-v3.1-terminus-thinking",
        "topic": {         
            "new.jsonl": 1,   
            "code.jsonl": 1,
            "Inference.jsonl": 1,
            "ComplexCommand.jsonl": 1,
            "Form.jsonl": 1,
            "Mathematical.jsonl": 1,
            "rag_new_question.json": 0,
            "aime24.jsonl": 0,
            "aime25.jsonl": 0
        },
        "model_name":"deepseek-v3.1-terminus-thinking",
        "enable_thinking": 1,
        "judgeByLLM": 0,
        "json2xlsx": 0,
        "type":"api",
        "run": 0
    },
```

convert主要就是各个文件形式之间的互相转换

models就是模型的调用方法（API，local,VLLM）等，和调用的模型进行封装，暴露给config进行调整

others，recyclle_bin,result，temp_test观察了下可能是半成品数据集。

topic就是原始数据集

work是处理好数据集的存储目录

main函数是程序主要运行入口，

judge)by_llm是数据集评测的方法，入口



### 项目main代码阅读
大概就是先加载好模型数据集，之后把数据集的question拿出来，用大模型标注answer，同时让大模型对标注的answer做个测评

#### 数据集处理
首先判断是哪个数据集

对于code.json

首先判断是否已经有生成完毕的数据集，如果生成数据集最后完成的时间要晚于原始数据集，则不需要更新



如果有把数据集与本数据集做个对比，只取原始数据集在生成数据集之后更新的部分重新生成





### 装了个环境跑qwen3
### 复现下searchr1
### 简单阅读了一下hello agent的前几个部分
### nvitop
[https://blog.csdn.net/Sep21m_wyy/article/details/141754651](https://blog.csdn.net/Sep21m_wyy/article/details/141754651)

## 10.23回顾
### 阅读公司系统手册
包括了权限，资源等，主要也就是权限的管理，以及进入各个系统/服务器的方法之类的，基本上一个<font style="color:rgb(15, 17, 21);background-color:rgb(237, 243, 254);">LDAP就都可以登陆上了，具体LDAP是什么放在下面</font>

<details class="lake-collapse"><summary id="u2a51019a"><span class="ne-text" style="font-size: 16px">LDAP</span></summary><p id="u9a8bfb5f" class="ne-p"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">LDAP（轻量级目录访问协议）是一种用于查询和修改分布式目录信息服务的开放标准协议。</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px"> 你可以把它想象成互联网时代的“电子电话簿”或“企业信息黄页”。</span></p></details>
以及创建云平台/云平台配置的一些知识，放在下面了

<details class="lake-collapse"><summary id="uc8627508"><span class="ne-text" style="font-size: 16px">DinD及优势</span></summary><p id="u88c65342" class="ne-p"><span class="ne-text" style="color: rgb(15, 17, 21); background-color: rgb(237, 243, 254); font-size: 16px">开发机可启用 Docker 容器（ DinD）功能，允许用户在开发机内（主容器）运行其他容器，从而提供更高的灵活性和隔离性。 </span></p><p id="u8ca62492" class="ne-p"><span class="ne-text" style="color: rgb(15, 17, 21); background-color: rgb(237, 243, 254); font-size: 16px">特别是在需要 GPU 资源的场景下，用户可以在开发机 DinD 环境中构建镜像，并创建挂载 GPU 的容器，测试容器化应用，并将构建好的镜像推送到租户镜像仓库。</span></p><p id="uc55416f8" class="ne-p"><span class="ne-text" style="color: rgb(15, 17, 21); background-color: rgb(237, 243, 254); font-size: 16px">有以下特性</span></p><ul class="ne-ul"><li id="u45224ee1" data-lake-index-type="0"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">对宿主机隔离</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">：所有你的Docker操作（构建、运行、测试）都被限制在“开发机”这个容器内部。你不会污染宿主机上的Docker环境，也不会影响到其他用户。</span></li><li id="u1c17a7b6" data-lake-index-type="0"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">环境一致性</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">：开发团队可以共享一个预配置好的“开发机”镜像，确保所有人的基础开发环境（包括Docker版本）完全一致，避免了“在我机器上能运行”的问题。</span></li><li id="ufaafbb63" data-lake-index-type="0"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">灵活测试</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">：你可以随意创建、启动、停止、删除多个内部容器，来模拟复杂的多服务应用（例如，一个Web前端容器 + 一个后端API容器 + 一个数据库容器），而所有这些都封装在属于你自己的那个“开发机”里。</span></li></ul><p id="u39cf9bb9" class="ne-p"><span class="ne-text" style="color: rgb(15, 17, 21)">GPU资源的使用</span></p><ul class="ne-ul"><li id="ubd7378b3" data-lake-index-type="0"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">痛点</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">：通常，Docker容器默认是无法使用宿主机上的GPU（显卡）的。需要特殊的配置和参数（如</span><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px"> </span><code class="ne-code"><span class="ne-text" style="color: rgb(15, 17, 21); background-color: rgb(235, 238, 242); font-size: 16px">--gpus all</span></code><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">）才能让容器访问GPU。</span></li><li id="u1e5a8ec4" data-lake-index-type="0"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">解决方案</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">：这个开发环境提供了DinD功能，并且</span><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">允许在内部创建的容器上挂载GPU</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">。</span></li></ul><p id="u22daab7d" class="ne-p"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">工作流程如下：</span></strong></p><ol class="ne-ol"><li id="ueb6e200d" data-lake-index-type="0"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">在开发机内构建镜像</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">：你在“开发机”容器里，编写Dockerfile，然后使用</span><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px"> </span><code class="ne-code"><span class="ne-text" style="color: rgb(15, 17, 21); background-color: rgb(235, 238, 242); font-size: 16px">docker build</span></code><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px"> </span><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">命令构建一个包含你AI模型和代码的</span><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">新镜像</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">。这个新镜像是从“开发机”内部的Docker引擎构建出来的。</span></li><li id="uef6678ad" data-lake-index-type="0"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">创建挂载GPU的容器</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">：然后，你使用</span><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px"> </span><code class="ne-code"><span class="ne-text" style="color: rgb(15, 17, 21); background-color: rgb(235, 238, 242); font-size: 16px">docker run --gpus all ...</span></code><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px"> </span><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">命令，从刚构建好的镜像</span><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">创建并运行一个内部容器</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">。这个命令会通过DinD的特殊配置，成功地将宿主机的GPU资源“穿透”到你这个内部容器中。</span></li><li id="u74e6bb73" data-lake-index-type="0"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">测试容器化应用</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">：现在，你这个内部容器既能隔离运行，又能直接调用GPU进行高速计算。你可以充分测试你的AI模型推理或训练过程。</span></li><li id="ub34605f8" data-lake-index-type="0"><strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">推送到镜像仓库</span></strong><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px">：测试无误后，你可以从“开发机”内部使用 </span><code class="ne-code"><span class="ne-text" style="color: rgb(15, 17, 21); background-color: rgb(235, 238, 242); font-size: 16px">docker push</span></code><span class="ne-text" style="color: rgb(15, 17, 21); font-size: 16px"> 命令，将这个构建好的、已验证可用的镜像推送到你的团队或个人的镜像仓库（如AWS ECR, Docker Hub, 私有Harbor等）。</span></li></ol></details>
之后就没太多需要熟悉了，leader带我配置了下新系统，linux基础命令要熟悉下了，有些基础命令没熟悉导致当时很多没搞懂，

### 新增系统用户并配置
1. 创建用户

```plain
sudo useradd -m -s /bin/bash sunyujie  # 创建用户并创建家目录
sudo passwd sunyujie                   # 设置密码
```

2. 给用户改名，作为备份

```plain
# 备份原来的家目录
mv sunyujie sunyujie_bak
```

3. 重新创建，并把共享文件夹挂载到用户目录下

```plain
# 在共享存储创建新目录
cd /mnt/public/algm/
mkdir sunyujie
ln -s /mnt/public/algm/sunyujie /home/sunyujie
```

4. 把原先用户目录的bashrc重新放到共享文件夹下，权限修改一下

```plain
# 修改共享目录的所有权
chown sunyujie:sunyujie /mnt/public/algm/sunyujie/

# 从备份中复制shell配置文件
cp /home/sunyujie_bak/.bashrc .
```

+ <font style="color:rgb(15, 17, 21);">让用户</font><font style="color:rgb(15, 17, 21);"> </font>`<font style="color:rgb(15, 17, 21);background-color:rgb(235, 238, 242);">sunyujie</font>`<font style="color:rgb(15, 17, 21);"> </font><font style="color:rgb(15, 17, 21);">使用</font><font style="color:rgb(15, 17, 21);"> </font>`<font style="color:rgb(15, 17, 21);background-color:rgb(235, 238, 242);">/mnt/public/algm/</font>`<font style="color:rgb(15, 17, 21);"> </font><font style="color:rgb(15, 17, 21);">下的共享存储空间</font>
+ <font style="color:rgb(15, 17, 21);">保持用户访问路径不变（仍然是 </font>`<font style="color:rgb(15, 17, 21);background-color:rgb(235, 238, 242);">/home/sunyujie</font>`<font style="color:rgb(15, 17, 21);">）</font>

### ssh连接服务器
ssh链接没啥问题，vscode配置恶心坏了,ds感觉幻觉不小，给我这胡乱尝试耽误了不少时间

#### ssh
+ 本地生成公钥和私钥
+ cmd调用ssh服务即可，默认在user/.ssh文件夹下读取公钥和私钥

#### vscode ssh
##### 失败尝试
根据日志的关键信息查询并尝试了：

1. **已有解决方案查询阶段**
    1. 把所有vscode的相关内容（服务器端的下载，进程，及其缓存全部清空，本地的vscode全部清空），之后重新ssh链接，让服务器重新下载
        1. ref：[https://blog.gitcode.com/504ce2714887180ed9b0d97cab5a0967.html](https://blog.gitcode.com/504ce2714887180ed9b0d97cab5a0967.html)
        2. ref: [https://blog.csdn.net/qq_38667212/article/details/140462083](https://blog.csdn.net/qq_38667212/article/details/140462083)
    2. 但是发现没有效果
2. **查询deepseek阶段**

deepseek给出的原因是：本地vscode ssh链接通之后，下载vscode-server，并重新链接回本机。所以很大可能是vscode-server下载失败或者无法匹配上本地vscode版本，以下的操作都围绕着这个过程去排查

    1. **排查是否因为服务器网络原因导致无法在服务器端下载vscode**

```plain
# 测试到 GitHub 和 VS Code 服务器的连通性
ping -c 3 raw.githubusercontent.com
ping -c 3 update.code.visualstudio.com
curl -I https://update.code.visualstudio.com
```

发现实际上都能够ping通

    2. **检查是不是在服务器端防火墙出现问题**

```plain
sudo ufw status
```

    3. **手动下载与本地vscode对应版本**

```plain
wget https://update.code.visualstudio.com/commit:ddc367ed5c8936efe395cffeec279b04ffd7db78/server-linux-x64/stable -O vscode-server-linux-x64.tar.gz
```

下载完之后验证是否版本一致，以及确认是否本用户拥有访问权限：

```plain
./bin/code-server --version
```

并检查安装完整性：

```plain
ls -la ~/.vscode-server/bin/ddc367ed5c8936efe395cffeec279b04ffd7db78/
```

    4. **发现并不行，之后重新尝试了把所有vscode的相关内容（服务器端的下载，进程，及其缓存全部清空，本地的vscode）全部清空，也就是回到step1，重新安装并尝试上述内容，依旧失败**

##### 成功方法
1. chatgpt重问

md一下给我搞出来了，vscode勾选了个enable框框就成功了，具体在公司电脑上不方便截图，如下

> 1. <font style="color:rgb(204, 204, 204);background-color:rgb(24, 24, 24);">回退旧流程：关闭“新引导模式”。这类“Failed to parse remote port”常由 exec server 引起。</font>
> 2. <font style="color:rgb(204, 204, 204);background-color:rgb(24, 24, 24);">清理远端 VS Code Server 后重连，避免旧版本/损坏安装影响。</font>
> 3. <font style="color:rgb(204, 204, 204);background-color:rgb(24, 24, 24);">若仍失败，改用“套接字监听”以绕过端口分配问题；按提示先终止远端服务器后再连。</font>
> 4. <font style="color:rgb(204, 204, 204);background-color:rgb(24, 24, 24);">提升日志并显示登录终端，重试后查看详细报错。</font>
>

### 总结：
总结下：

1. 均匀尝试，一个看着解决复杂就要换另一个搜索源看看
2. ssh的基础知识
3. linux系统操作命令大概也就分为对文件的增删改查，文件关系的维护，和对权限的增删改查。
    1. mv 移动



## 
